{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSources:\\nhttps://forums.ni.com/t5/LabVIEW/regex-find-number-at-end-of-string/td-p/2510826?profile.language=en\\nhttps://numpy.org/devdocs/user/basics.types.html\\nhttps://www.geeksforgeeks.org/numpy-floor-python/\\nhttps://stackoverflow.com/questions/12201577/how-can-i-convert-an-rgb-image-into-grayscale-in-python\\nhttps://www.tutorialspoint.com/python/python_multithreading.htm\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sources:\n",
    "https://forums.ni.com/t5/LabVIEW/regex-find-number-at-end-of-string/td-p/2510826?profile.language=en\n",
    "https://numpy.org/devdocs/user/basics.types.html\n",
    "https://www.geeksforgeeks.org/numpy-floor-python/\n",
    "https://stackoverflow.com/questions/12201577/how-can-i-convert-an-rgb-image-into-grayscale-in-python\n",
    "https://www.tutorialspoint.com/python/python_multithreading.htm\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import concurrent.futures\n",
    "\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import PatternMatchingEventHandler\n",
    "from watchdog.events import RegexMatchingEventHandler\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from skimage.future import graph\n",
    "import skimage.data as data\n",
    "import skimage.segmentation as seg\n",
    "import skimage.filters as filters\n",
    "import skimage.draw as draw\n",
    "import skimage.color as color\n",
    "from skimage import img_as_float\n",
    "from skimage import exposure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import io\n",
    "import skimage.restoration as restore\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import log10, sqrt\n",
    "\n",
    "import os\n",
    "from os import walk\n",
    "from os import listdir\n",
    "\n",
    "from PIL import Image, ImageFilter, ImageFile\n",
    "\n",
    "import shutil\n",
    "\n",
    "from DirectoryGenerator import DirectoryGenerator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import multiprocessing as mp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateAOIBoundingBoxes(imgDirectory, tempDirectory, finalDirectory, paramList, debugMode=False):\n",
    "    def readImage(method, fileName):\n",
    "        if(method == 'cv2'):\n",
    "            return cv2.imread(fileName, cv2.IMREAD_GRAYSCALE)\n",
    "        elif(method == 'skimage'):\n",
    "            img = io.imread(fileName)\n",
    "            return rgb2gray(img)\n",
    "        elif(method == 'pil'):\n",
    "            return Image.open(fileName).convert('LA')\n",
    "        \n",
    "    def saveImage(method, img, fileName):\n",
    "        if(method == 'cv2'):\n",
    "            cv2.imwrite(fileName, img)\n",
    "        elif(method == 'skimage'):\n",
    "            io.imsave(fileName, img_as_ubyte(np.uint8(np.floor(img))))\n",
    "        elif(method == 'pil'):\n",
    "            img.save(fileName)\n",
    "    \n",
    "    def sharpenImage(img, n=1):\n",
    "        # Create our shapening kernel, it must equal to one eventually\n",
    "        res = img\n",
    "        for i in range(0, n):\n",
    "            res = res.filter(ImageFilter.SHARPEN)\n",
    "        return res\n",
    "    \n",
    "    def histogramEqualization(img):\n",
    "        return exposure.equalize_hist(img) * 255\n",
    "        \n",
    "    def kMeansImageSegmentation(img, n_clusters=30, random_state=0):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(img)\n",
    "        pic2show = kmeans.cluster_centers_[kmeans.labels_]\n",
    "        cluster_pic = pic2show.reshape(img.shape[0], img.shape[1])\n",
    "        return cluster_pic\n",
    "    \n",
    "    def bilateralFilter(img, d=9, sigmaColor=500, sigmaSpace=1000):\n",
    "        return cv2.bilateralFilter(img, d=d, sigmaColor=sigmaColor, sigmaSpace=sigmaSpace)\n",
    "    \n",
    "    def slic(img, compactness=2000, n_segments=8000):\n",
    "        # Perform SLIC\n",
    "        labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
    "        return color.label2rgb(labels, img, kind='avg')\n",
    "\n",
    "    img = readImage('pil', imgDirectory)\n",
    "    img_sharp = sharpenImage(img, n=paramList['imgSharpening']['n'])\n",
    "    saveImage('pil', img_sharp, tempDirectory + '_sharp.png' if debugMode else tempDirectory)\n",
    "    \n",
    "    img_sharp = readImage('cv2', tempDirectory + '_sharp.png' if debugMode else tempDirectory)\n",
    "    img_hist_eq = histogramEqualization(img_sharp)\n",
    "    saveImage('cv2', img_hist_eq, tempDirectory + '_histeq.png' if debugMode else tempDirectory)\n",
    "    \n",
    "    img_hist_eq = readImage('skimage', tempDirectory + '_histeq.png' if debugMode else tempDirectory)\n",
    "    img_kmeans = kMeansImageSegmentation(img_hist_eq / 255, n_clusters=paramList['kmeans_1']['n_clusters'], random_state=paramList['kmeans_1']['random_state'])\n",
    "    saveImage('skimage', img_kmeans * 255, tempDirectory + '_kmeans.png' if debugMode else tempDirectory)\n",
    "    \n",
    "    img_kmeans = readImage('cv2', tempDirectory + '_kmeans.png' if debugMode else tempDirectory)\n",
    "    img_blur = bilateralFilter(img_kmeans, d=paramList['bilateralFilter']['d'], sigmaColor=paramList['bilateralFilter']['sigmaColor'], sigmaSpace=paramList['bilateralFilter']['sigmaSpace'])\n",
    "    saveImage('cv2', img_blur, tempDirectory + '_blur.png' if debugMode else tempDirectory)\n",
    "    \n",
    "    img_blur = readImage('skimage', tempDirectory + '_blur.png' if debugMode else tempDirectory)\n",
    "    img_slic = slic(img_blur, compactness=paramList['slic']['compactness'], n_segments=paramList['slic']['n_segments'])\n",
    "    saveImage('skimage', img_slic, tempDirectory + '_slic.png' if debugMode else tempDirectory)\n",
    "    \n",
    "    img_slic = readImage('skimage', tempDirectory + '_slic.png' if debugMode else tempDirectory)\n",
    "    img_slic_kmeans = kMeansImageSegmentation(img_slic / 255, n_clusters=paramList['kmeans_2']['n_clusters'], random_state=paramList['kmeans_2']['random_state'])\n",
    "    saveImage('skimage', img_slic_kmeans * 255, finalDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sceneSegmenterSSIM(videoFilePath, resultFolderName, sceneRefFolderName, threshold=0.70, recentFrameCompThreshold=0.99, framesPerSecond=1, totalFrames=0, useHeuristics=False):\n",
    "    # Get the video file\n",
    "    vidObj = cv2.VideoCapture(videoFilePath)\n",
    "    # Track the number of frames read so far\n",
    "    imageCount = 0\n",
    "    # Track the number of scenes created so far\n",
    "    sceneCount = 0\n",
    "    # This array is used for storing the normalized representation of the first frame element of each scene\n",
    "    # for the comparison process\n",
    "    sceneRef = []\n",
    "    recentFrame = None\n",
    "    recentPredScene = None\n",
    "    \n",
    "    def convertToGrayscale(image):\n",
    "        # Convert the current frame to grayscale\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    def normalizeAdaptiveThreshold(imageGray):\n",
    "        # Normalize the grayscale representation of the current frame using cv2.adaptiveThreshold\n",
    "        # with cv2.ADAPTIVE_THRESH_GAUSSIAN_C and cv2.THRESH_BINARY\n",
    "        return cv2.adaptiveThreshold(imageGray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n",
    "    \n",
    "    def predictSceneRecent(imageNorm):\n",
    "        nonlocal sceneRef\n",
    "        nonlocal imageCount\n",
    "        nonlocal recentPredScene\n",
    "        nonlocal recentFrame\n",
    "        nonlocal recentFrameCompThreshold\n",
    "        \n",
    "        ssimVal = None\n",
    "        isPart = False\n",
    "        \n",
    "        if(recentPredScene != None and recentFrame.all() != None):\n",
    "            ssimVal = ssim(imageNorm, recentFrame)\n",
    "            isPart = ssimVal >= recentFrameCompThreshold\n",
    "            \n",
    "            logFile = open('./' + resultFolderName + '/ssimLogs.txt', \"a+\")\n",
    "            logInput = str(imageCount)\n",
    "            logInput = logInput + \" recentPredScene: \" + str(recentPredScene) + \" ssim: \" + str(ssimVal) + \" result: \" + str(isPart) + \"\\n\"\n",
    "            logFile.write(logInput)\n",
    "            logFile.close()\n",
    "        \n",
    "        return isPart\n",
    "    \n",
    "    def predictScene(imageNorm):\n",
    "        nonlocal sceneRef\n",
    "        nonlocal imageCount\n",
    "        \n",
    "        ssimResults = [ssim(imageNorm, scene) for scene in sceneRef]\n",
    "        \n",
    "        maxVal = None\n",
    "        maxScene = None\n",
    "        \n",
    "        if(len(ssimResults) > 0):\n",
    "            maxVal = max(ssimResults)\n",
    "            maxScene = ssimResults.index(maxVal)\n",
    "        \n",
    "        logFile = open('./' + resultFolderName + '/ssimLogs.txt', \"a+\")\n",
    "        logInput = str(imageCount)\n",
    "        for i in range(0, len(ssimResults)):\n",
    "            logInput = logInput + \" \" + str(i) + \": \" + str(ssimResults[i])\n",
    "        logInput = logInput + \" maxVal: \" + str(maxVal) + \" predScene: \" + str(maxScene) + \"\\n\"\n",
    "        logFile.write(logInput)\n",
    "        logFile.close()\n",
    "        \n",
    "        return maxScene, maxVal\n",
    "    \n",
    "    def storeRecentScene(imageOrig, imageGray, imageNorm):\n",
    "        nonlocal imageCount\n",
    "        nonlocal sceneCount\n",
    "        nonlocal resultFolderName\n",
    "        nonlocal recentPredScene\n",
    "        \n",
    "        # Just save the grayscale representation of the current frame in the folder\n",
    "        # pertaining to the predicted scene of that frame\n",
    "        # as a PNG file with its file name being the current frame count\n",
    "        cv2.imwrite('./' + resultFolderName + '/' + str(recentPredScene) + '/orig/frame%d.png' % imageCount, imageOrig)\n",
    "        cv2.imwrite('./' + resultFolderName + '/' + str(recentPredScene) + '/gray/frame%d.png' % imageCount, imageGray)\n",
    "        cv2.imwrite('./' + resultFolderName + '/' + str(recentPredScene) + '/norm/frame%d.png' % imageCount, imageNorm)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def storeScene(imageOrig, imageGray, imageNorm, maxScene, maxVal):\n",
    "        nonlocal threshold\n",
    "        nonlocal imageCount\n",
    "        nonlocal sceneCount\n",
    "        nonlocal sceneRef\n",
    "        nonlocal resultFolderName\n",
    "        nonlocal recentPredScene\n",
    "        nonlocal recentFrame\n",
    "        \n",
    "        # If the ssim value of the predicted scene of the current frame is less than the threshold value\n",
    "        if(maxVal == None or maxVal < threshold):\n",
    "            # Create a subfolder in \"segmented_data_SSIM\" named after the current no. of scenes\n",
    "            # if it does not exists\n",
    "            try:\n",
    "                os.mkdir('./' + resultFolderName + '/' + str(sceneCount))\n",
    "                os.mkdir('./' + resultFolderName + '/' + str(sceneCount) + '/orig')\n",
    "                os.mkdir('./' + resultFolderName + '/' + str(sceneCount) + '/gray')\n",
    "                os.mkdir('./' + resultFolderName + '/' + str(sceneCount) + '/norm')\n",
    "            except(FileExistsError):\n",
    "                print('./' + resultFolderName + '/' + str(sceneCount) + ' Exists')\n",
    "            # Save the grayscale representation of the current frame in that folder\n",
    "            # as a PNG file with its file name being the current frame count\n",
    "            cv2.imwrite('./' + resultFolderName + '/' + str(sceneCount) + '/orig/frame%d.png' % imageCount, imageOrig)\n",
    "            cv2.imwrite('./' + resultFolderName + '/' + str(sceneCount) + '/gray/frame%d.png' % imageCount, imageGray)\n",
    "            cv2.imwrite('./' + resultFolderName + '/' + str(sceneCount) + '/norm/frame%d.png' % imageCount, imageNorm)\n",
    "            # Append the normalized representation of the current frame to sceneRef\n",
    "            sceneRef.append(imageNorm)\n",
    "            # Increment the number of scenes\n",
    "            recentPredScene = sceneCount\n",
    "            sceneCount = sceneCount + 1\n",
    "            try:\n",
    "                os.mkdir('./' + sceneRefFolderName + '/' + str(sceneCount))\n",
    "            except(FileExistsError):\n",
    "                print('./' + sceneRefFolderName + '/' + str(sceneCount) + ' Exists')\n",
    "            for i in range(0, len(sceneRef)):\n",
    "                cv2.imwrite('./' + sceneRefFolderName + '/' + str(sceneCount) + '/frame%d_.png' % i, sceneRef[i])\n",
    "        # Otherwise\n",
    "        else:\n",
    "            # Just save the grayscale representation of the current frame in the folder\n",
    "            # pertaining to the predicted scene of that frame\n",
    "            # as a PNG file with its file name being the current frame count\n",
    "            cv2.imwrite('./' + resultFolderName + '/' + str(maxScene) + '/orig/frame%d.png' % imageCount, imageOrig)\n",
    "            cv2.imwrite('./' + resultFolderName + '/' + str(maxScene) + '/gray/frame%d.png' % imageCount, imageGray)\n",
    "            cv2.imwrite('./' + resultFolderName + '/' + str(maxScene) + '/norm/frame%d.png' % imageCount, imageNorm)\n",
    "            recentPredScene = maxScene\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # While there are still frames to be read in the video file\n",
    "    # Note: I added imageCount < 200 for the purpose of testing\n",
    "    success = 1\n",
    "    while(success and (imageCount < totalFrames if totalFrames > 0 else True)):\n",
    "        # Obtain the next frame in the video file\n",
    "        success, image = vidObj.read()\n",
    "        \n",
    "        if(imageCount % framesPerSecond == 0):\n",
    "            imageGray = convertToGrayscale(image)\n",
    "            imageNorm = normalizeAdaptiveThreshold(imageGray)\n",
    "            if(useHeuristics):\n",
    "                isPartOfRecentScene = predictSceneRecent(imageNorm)\n",
    "                if(isPartOfRecentScene):\n",
    "                    storeRecentScene(image, imageGray, imageNorm)\n",
    "                else:\n",
    "                    maxScene, maxVal = predictScene(imageNorm)\n",
    "                    storeScene(image, imageGray, imageNorm, maxScene, maxVal)\n",
    "                recentFrame = imageNorm\n",
    "            else:\n",
    "                maxScene, maxVal = predictScene(imageNorm)\n",
    "                storeScene(image, imageGray, imageNorm, maxScene, maxVal)\n",
    "        \n",
    "        # Increment the number of frames read\n",
    "        imageCount = imageCount + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# http://brunorocha.org/python/watching-a-directory-for-file-changes-with-python.html\n",
    "# https://blog.magrathealabs.com/filesystem-events-monitoring-with-python-9f5329b651c3\n",
    "# https://www.reddit.com/r/learnpython/comments/7kv2pc/using_watchdog_to_monitor_more_than_one_directory/\n",
    "# https://stackoverflow.com/questions/17756011/how-to-match-only-particular-events-with-python-watchdog\n",
    "# https://realpython.com/intro-to-python-threading/\n",
    "\n",
    "def runVideoFrameSceneGrouping(videoFrameGroupingParams):\n",
    "    folderObservers = []\n",
    "    sceneObservers = []\n",
    "    failedFrames = {}\n",
    "    numberOfFrames = 0\n",
    "    \n",
    "    def AOIEstimationFailSafeThread(files):\n",
    "        for file in files:\n",
    "            imgDirectory = file\n",
    "            tempDirectory = './' + imgDirectory.split('/')[0] + '/' + imgDirectory.split('/')[1] + '/temp/' + imgDirectory.split('/')[3]\n",
    "            finalDirectory = './' + imgDirectory.split('/')[0] + '/' + imgDirectory.split('/')[1] + '/final/' + imgDirectory.split('/')[3]\n",
    "            estimateAOIBoundingBoxes(imgDirectory, tempDirectory, finalDirectory, videoFrameGroupingParams['AOIEstimationParams'], videoFrameGroupingParams['AOIEstimationParams']['debugMode'])\n",
    "            time.sleep(2)\n",
    "    \n",
    "    class SceneFolderHandler(PatternMatchingEventHandler):\n",
    "        patterns = ['*.png']\n",
    "        \n",
    "        def setFolderName(self, folderName):\n",
    "            self.folderName = folderName\n",
    "    \n",
    "        def incrementNumberOfFrames(self, lock):\n",
    "            nonlocal numberOfFrames\n",
    "            \n",
    "            lock.acquire()\n",
    "            numberOfFrames = numberOfFrames + 1\n",
    "            lock.release()\n",
    "        \n",
    "        def process(self, event):\n",
    "            nonlocal videoFrameGroupingParams\n",
    "            nonlocal failedFrames\n",
    "            \"\"\"\n",
    "            event.event_type \n",
    "                'modified' | 'created' | 'moved' | 'deleted'\n",
    "            event.is_directory\n",
    "                True | False\n",
    "            event.src_path\n",
    "                path/to/observed/file\n",
    "            \"\"\"\n",
    "            # the file will be processed there\n",
    "            try:\n",
    "                if(videoFrameGroupingParams['AOIFailSafeDebugOn']):\n",
    "                    failedFrames[self.folderName].append(str(event.src_path))\n",
    "                else:\n",
    "                    imgDirectory = str(event.src_path)\n",
    "                    tempDirectory = './' + imgDirectory.split('/')[0] + '/' + imgDirectory.split('/')[1] + '/temp/' + imgDirectory.split('/')[3]\n",
    "                    finalDirectory = './' + imgDirectory.split('/')[0] + '/' + imgDirectory.split('/')[1] + '/final/' + imgDirectory.split('/')[3]\n",
    "                    estimateAOIBoundingBoxes(imgDirectory, tempDirectory, finalDirectory, videoFrameGroupingParams['AOIEstimationParams'], videoFrameGroupingParams['AOIEstimationParams']['debugMode'])\n",
    "            except(Exception):\n",
    "                failedFrames[self.folderName].append(str(event.src_path))\n",
    "            finally:\n",
    "                self.incrementNumberOfFrames(threading.Lock())\n",
    "        \n",
    "        def on_created(self, event):\n",
    "            self.process(event)\n",
    "            \n",
    "    class MainFolderHandler(PatternMatchingEventHandler):\n",
    "        def setFolderName(self, folderName):\n",
    "            self.folderName = folderName\n",
    "\n",
    "        def process(self, event):\n",
    "            nonlocal sceneObservers\n",
    "            nonlocal failedFrames\n",
    "            \"\"\"\n",
    "            event.event_type \n",
    "                'modified' | 'created' | 'moved' | 'deleted'\n",
    "            event.is_directory\n",
    "                True | False\n",
    "            event.src_path\n",
    "                path/to/observed/file\n",
    "            \"\"\"\n",
    "            # the file will be processed there\n",
    "            if(event.is_directory):\n",
    "                logFile = open('./' + str(self.folderName) + '.txt', \"a+\")\n",
    "                logInput = str(event.src_path) + \"/gray\"\n",
    "                logFile.write(logInput + \"\\n\")\n",
    "                logFile.close()\n",
    "                \n",
    "                try:\n",
    "                    os.mkdir('./' + str(event.src_path) + '/temp')\n",
    "                    os.mkdir('./' + str(event.src_path) + '/final')\n",
    "                except(FileExistsError):\n",
    "                    print('./' + str(event.src_path) + '/temp' + ' Exists')\n",
    "                \n",
    "                failedFrames[logInput] = []\n",
    "            \n",
    "                sceneObservers.append(Observer())\n",
    "            \n",
    "                handler = SceneFolderHandler()\n",
    "                handler.setFolderName(logInput)\n",
    "            \n",
    "                sceneObservers[-1].schedule(handler, path=logInput)\n",
    "                sceneObservers[-1].start()\n",
    "            \n",
    "        def on_created(self, event):\n",
    "            self.process(event)\n",
    "    \n",
    "    folderObservers.append(Observer())\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('./' + videoFrameGroupingParams['resultFolderName'])\n",
    "        os.mkdir('./' + videoFrameGroupingParams['resultFolderName'] + '/sceneRef')\n",
    "    except(FileExistsError):\n",
    "        print(\"Folder Exists!\")\n",
    "    \n",
    "    logFile = open('./' + str(videoFrameGroupingParams['resultFolderName']) + '.txt', \"a+\")\n",
    "    logFile.close()\n",
    "    \n",
    "    handler = MainFolderHandler()\n",
    "    handler.setFolderName(videoFrameGroupingParams['resultFolderName'])\n",
    "    \n",
    "    folderObservers[-1].schedule(handler, path=videoFrameGroupingParams['resultFolderName'])\n",
    "    folderObservers[-1].start()\n",
    "    \n",
    "    sceneSegmenterSSIM(videoFrameGroupingParams['video'], videoFrameGroupingParams['resultFolderName'], videoFrameGroupingParams['resultFolderName'] + '/sceneRef', videoFrameGroupingParams['threshold'], videoFrameGroupingParams['recentFrameCompThreshold'], videoFrameGroupingParams['framesPerSecond'], videoFrameGroupingParams['totalFrames'], videoFrameGroupingParams['useHeuristics'])\n",
    "    \n",
    "    try:\n",
    "        while(numberOfFrames < videoFrameGroupingParams['totalFrames'] if videoFrameGroupingParams['totalFrames'] > 0 else True):\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Process Interrupted!')\n",
    "    finally:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=len(failedFrames.values())) as executor:\n",
    "            executor.map(AOIEstimationFailSafeThread, failedFrames.values())\n",
    "        for folderObserver in folderObservers:\n",
    "            folderObserver.unschedule_all()\n",
    "            folderObserver.stop()\n",
    "        for sceneObserver in sceneObservers:\n",
    "            sceneObserver.unschedule_all()\n",
    "            sceneObserver.stop()\n",
    "            \n",
    "    return failedFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameSceneGroupingMultipleVideos:\n",
    "    def __init__(this):\n",
    "        this.videoList = {}\n",
    "    \n",
    "    def addVideo(this, video, resultFolderName):\n",
    "        this.videoList[resultFolderName] = {\n",
    "            'video': video,\n",
    "            'resultFolderName': resultFolderName,\n",
    "            'AOIFailSafeDebugOn': False,\n",
    "            'threshold': 0.70,\n",
    "            'recentFrameCompThreshold': 0.99,\n",
    "            'framesPerSecond': 1,\n",
    "            'totalFrames': 0,\n",
    "            'useHeuristics': False,\n",
    "            'AOIEstimationParams': {\n",
    "                'imgSharpening': {\n",
    "                    'n': 1\n",
    "                },\n",
    "                'kmeans_1': {\n",
    "                    'n_clusters': 30,\n",
    "                    'random_state': 0\n",
    "                },\n",
    "                'bilateralFilter': {\n",
    "                    'd': 9,\n",
    "                    'sigmaColor': 500,\n",
    "                    'sigmaSpace': 1000\n",
    "                },\n",
    "                'slic': {\n",
    "                    'compactness': 2000,\n",
    "                    'n_segments': 8000\n",
    "                },\n",
    "                'kmeans_2': {\n",
    "                    'n_clusters': 30,\n",
    "                    'random_state': 0\n",
    "                },\n",
    "                'debugMode': False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def setAOIFailSafeDebugOn(this, resultFolderName, AOIFailSafeDebugOn):\n",
    "        this.videoList[resultFolderName]['AOIFailSafeDebugOn'] = bool(AOIFailSafeDebugOn)\n",
    "    \n",
    "    def setThreshold(this, resultFolderName, threshold):\n",
    "        this.videoList[resultFolderName]['threshold'] = float(threshold)\n",
    "        \n",
    "    def setRecentFrameCompThreshold(this, resultFolderName, recentFrameCompThreshold):\n",
    "        this.videoList[resultFolderName]['recentFrameCompThreshold'] = float(recentFrameCompThreshold)\n",
    "        \n",
    "    def setFramesPerSecond(this, resultFolderName, framesPerSecond):\n",
    "        this.videoList[resultFolderName]['framesPerSecond'] = int(framesPerSecond)\n",
    "        \n",
    "    def setTotalFrames(this, resultFolderName, totalFrames):\n",
    "        this.videoList[resultFolderName]['totalFrames'] = int(totalFrames)\n",
    "        \n",
    "    def setUseHeuristics(this, resultFolderName, useHeuristics):\n",
    "        this.videoList[resultFolderName]['useHeuristics'] = bool(useHeuristics)\n",
    "        \n",
    "    def setImgSharpeningN(this, resultFolderName, n):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['imgSharpening']['n'] = int(n)\n",
    "    \n",
    "    def setKMeans1NClusters(this, resultFolderName, n_clusters):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['kmeans_1']['n_clusters'] = int(n_clusters)\n",
    "    \n",
    "    def setKMeans1RandomState(this, resultFolderName, random_state):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['kmeans_1']['random_state'] = int(random_state)\n",
    "        \n",
    "    def setBilateralFilterD(this, resultFolderName, d):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['bilateralFilter']['d'] = int(d)\n",
    "        \n",
    "    def setBilateralFilterSigmaColor(this, resultFolderName, sigmaColor):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['bilateralFilter']['sigmaColor'] = int(sigmaColor)\n",
    "        \n",
    "    def setBilateralFilterSigmaSpace(this, resultFolderName, sigmaSpace):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['bilateralFilter']['sigmaSpace'] = int(sigmaSpace)\n",
    "        \n",
    "    def setSlicCompactness(this, resultFolderName, compactness):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['slic']['compactness'] = int(compactness)\n",
    "        \n",
    "    def setSlicNSegments(this, resultFolderName, n_segments):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['slic']['n_segments'] = int(n_segments)\n",
    "        \n",
    "    def setKMeans2NClusters(this, resultFolderName, n_clusters):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['kmeans_2']['n_clusters'] = int(n_clusters)\n",
    "    \n",
    "    def setKMeans2RandomState(this, resultFolderName, random_state):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['kmeans_2']['random_state'] = int(random_state)\n",
    "        \n",
    "    def setDebugMode(this, resultFolderName, debugMode):\n",
    "        this.videoList[resultFolderName]['AOIEstimationParams']['debugMode'] = bool(debugMode)\n",
    "        \n",
    "    def runVideoFrameSceneGrouping(this):\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        print(\"Number of processors: \", mp.cpu_count())\n",
    "        videoSeg = np.array(pool.map(runVideoFrameSceneGrouping, this.videoList.values()))\n",
    "        pool.close()\n",
    "        \n",
    "        print(str(videoSeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameSceneGroupingParams:\n",
    "    def __init__(this, video, resultFolderName):\n",
    "        this.video = str(video)\n",
    "        this.resultFolderName = str(resultFolderName)\n",
    "        this.threshold = 0.70\n",
    "        this.recentFrameCompThreshold = 0.99\n",
    "        this.framesPerSecond = 1\n",
    "        this.totalFrames = 850\n",
    "        this.useHeuristics = False\n",
    "    \n",
    "    def setThreshold(this, threshold):\n",
    "        this.threshold = float(threshold)\n",
    "    \n",
    "    def setRecentFrameCompThreshold(this, recentFrameCompThreshold):\n",
    "        this.recentFrameCompThreshold = float(recentFrameCompThreshold)\n",
    "        \n",
    "    def setFramesPerSecond(this, framesPerSecond):\n",
    "        this.framesPerSecond = int(framesPerSecond)\n",
    "    \n",
    "    def setTotalFrames(this, totalFrames):\n",
    "        this.totalFrames = int(totalFrames)\n",
    "    \n",
    "    def setUseHeuristics(this, useHeuristics):\n",
    "        this.useHeuristics = bool(useHeuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runVideoFrameSceneGroupingOnly(videoFrameGroupingParams):\n",
    "    try:\n",
    "        os.mkdir('./' + videoFrameGroupingParams.resultFolderName)\n",
    "        os.mkdir('./' + videoFrameGroupingParams.resultFolderName + '/sceneRef')\n",
    "    except(FileExistsError):\n",
    "        print(\"Folder Exists!\")\n",
    "    \n",
    "    start = time.time()\n",
    "    sceneSegmenterSSIM(videoFrameGroupingParams.video, videoFrameGroupingParams.resultFolderName, videoFrameGroupingParams.resultFolderName + '/sceneRef', videoFrameGroupingParams.threshold, videoFrameGroupingParams.recentFrameCompThreshold, videoFrameGroupingParams.framesPerSecond, videoFrameGroupingParams.totalFrames, videoFrameGroupingParams.useHeuristics)\n",
    "    end = time.time()\n",
    "    logFile = open('./' + videoFrameGroupingParams.resultFolderName + '/ssimLogs.txt', \"a+\")\n",
    "    logFile.write(\"Time Taken: \" + str(end - start))\n",
    "    logFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVideoFrameSceneGroupingTValue():\n",
    "    testVideos = [VideoFrameSceneGroupingParams('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_t5'),\n",
    "                  VideoFrameSceneGroupingParams('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_t9'),\n",
    "                  VideoFrameSceneGroupingParams('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_t7')]\n",
    "\n",
    "    testVideos[0].setThreshold(0.5)\n",
    "    testVideos[1].setThreshold(0.9)\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    print(\"Number of processors: \", mp.cpu_count())\n",
    "    videoSeg = np.array(pool.map(runVideoFrameSceneGroupingOnly, testVideos))\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVideoFrameSceneGroupingHeuristicBenchmark():\n",
    "    testVideos = [VideoFrameSceneGroupingParams('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_nonheuristic_benchmark'),\n",
    "                  VideoFrameSceneGroupingParams('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_heuristic_99_benchmark')]\n",
    "    testVideos[1].setUseHeuristics(True)\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    print(\"Number of processors: \", mp.cpu_count())\n",
    "    videoSeg = np.array(pool.map(runVideoFrameSceneGroupingOnly, testVideos))\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVideoFrameSceneGroupingHeuristicBenchmarkTwo():\n",
    "    testVideos = [VideoFrameSceneGroupingParams('ADDU-DI02 video_export_01-23-17-19.33.08.avi', 'ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_nonheuristic_benchmark'),\n",
    "                  VideoFrameSceneGroupingParams('ADDU-DI02 video_export_01-23-17-19.33.08.avi', 'ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_heuristic_99_benchmark')]\n",
    "    testVideos[1].setUseHeuristics(True)\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    print(\"Number of processors: \", mp.cpu_count())\n",
    "    videoSeg = np.array(pool.map(runVideoFrameSceneGroupingOnly, testVideos))\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVideoFrameSceneGroupingCrossVideo():\n",
    "    testVideos = [VideoFrameSceneGroupingParams('ADDU-DI02 video_export_01-23-17-19.33.08.avi', 'ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_test'), \n",
    "                  VideoFrameSceneGroupingParams('ADDU-DI03 video_export_01-23-17-15.48.12.avi', 'ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_test'), \n",
    "                  VideoFrameSceneGroupingParams('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_test')]\n",
    "    for i in range(0, len(testVideos)):\n",
    "        testVideos[i].setUseHeuristics(True)\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    print(\"Number of processors: \", mp.cpu_count())\n",
    "    videoSeg = np.array(pool.map(runVideoFrameSceneGroupingOnly, testVideos))\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testVideoFrameSceneGroupingSingleVideo():\n",
    "    test = VideoFrameSceneGroupingParams('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest')\n",
    "    test.setUseHeuristics(True)\n",
    "    \n",
    "    runVideoFrameSceneGroupingOnly(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAutomaticAOIBoundingBoxesEstimationAlgorithmDetailed():\n",
    "    sample_data = ['./' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/0/gray/frame0.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/1/gray/frame18.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/2/gray/frame125.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/3/gray/frame139.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/6/gray/frame803.png']\n",
    "    paramList = {\n",
    "                    'imgSharpening': {\n",
    "                        'n': 1\n",
    "                    },\n",
    "                    'kmeans_1': {\n",
    "                        'n_clusters': 30,\n",
    "                        'random_state': 0\n",
    "                    },\n",
    "                    'bilateralFilter': {\n",
    "                        'd': 9,\n",
    "                        'sigmaColor': 500,\n",
    "                        'sigmaSpace': 1000\n",
    "                    },\n",
    "                    'slic': {\n",
    "                        'compactness': 2000,\n",
    "                        'n_segments': 8000\n",
    "                    },\n",
    "                    'kmeans_2': {\n",
    "                        'n_clusters': 30,\n",
    "                        'random_state': 0\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    for sample in sample_data:\n",
    "        imgFileName = sample\n",
    "        tempFileName = './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/' + (imgFileName.split('/')[-1]).split('.')[0] + '_temp.png'\n",
    "        finalFileName = './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/' + (imgFileName.split('/')[-1]).split('.')[0] + '_aoi.png'\n",
    "        estimateAOIBoundingBoxes(imgFileName, tempFileName, finalFileName, paramList, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAutomaticAOIBoundingBoxesEstimationAlgorithmBenchmark():\n",
    "    #sample_data = ['./' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/0/gray/frame0.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/1/gray/frame18.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/2/gray/frame125.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/3/gray/frame139.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/6/gray/frame803.png']\n",
    "    rootFolder = str(input())\n",
    "    sample_data = [rootFolder + DirectoryGenerator().getDelimiter() + f for f in os.listdir(rootFolder) if f.endswith('.png')]\n",
    "    paramList = {\n",
    "                    'imgSharpening': {\n",
    "                        'n': 1\n",
    "                    },\n",
    "                    'kmeans_1': {\n",
    "                        'n_clusters': 30,\n",
    "                        'random_state': 0\n",
    "                    },\n",
    "                    'bilateralFilter': {\n",
    "                        'd': 9,\n",
    "                        'sigmaColor': 500,\n",
    "                        'sigmaSpace': 1000\n",
    "                    },\n",
    "                    'slic': {\n",
    "                        'compactness': 2000,\n",
    "                        'n_segments': 8000\n",
    "                    },\n",
    "                    'kmeans_2': {\n",
    "                        'n_clusters': 30,\n",
    "                        'random_state': 0\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    for sample in sample_data:\n",
    "        imgFileName = sample\n",
    "        #tempFileName = './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/' + (imgFileName.split('/')[-1]).split('.')[0] + '_temp.png'\n",
    "        #finalFileName = './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/' + (imgFileName.split('/')[-1]).split('.')[0] + '_aoi.png'\n",
    "        frameName = (imgFileName.split(DirectoryGenerator().getDelimiter())[-1]).split('.')[0]\n",
    "        tempFileName = frameName + '_temp.png'\n",
    "        finalFileName = frameName + '_aoi.png'\n",
    "        \n",
    "        start = time.time()\n",
    "        estimateAOIBoundingBoxes(imgFileName, tempFileName, finalFileName, paramList, False)\n",
    "        end = time.time()\n",
    "        \n",
    "        logFile = open('AutomaticAOIBoundingBoxesEstimationBenchmarks_Old.txt', \"a+\")\n",
    "        logFile.write(\"Time Taken for \" + frameName + \": \" + str(end - start) + \"\\n\")\n",
    "        logFile.close()\n",
    "    \n",
    "    logFile = open('AutomaticAOIBoundingBoxesEstimationBenchmarks_Old.txt', \"a+\")\n",
    "    logFile.write(\"\\n\")\n",
    "    logFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://www.geeksforgeeks.org/python-peak-signal-to-noise-ratio-psnr/\n",
    "# https://intellipaat.com/community/1269/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\n",
    "\n",
    "def testAutomaticAOIBoundingBoxesEstimationAlgorithmPSNR():\n",
    "    def RMSE(imgOrig, imgSeg):\n",
    "        return sqrt(mean_squared_error(imgOrig, imgSeg))\n",
    "    \n",
    "    def PSNR(imgOrig, imgSeg): \n",
    "        mse = mean_squared_error(imgOrig, imgSeg)\n",
    "        if(mse == 0):\n",
    "            return 100\n",
    "        max_pixel = 255.0\n",
    "        psnr = 20 * log10(max_pixel / sqrt(mse)) \n",
    "        return psnr \n",
    "    \n",
    "    sample_data = ['./' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/0/gray/frame0.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/1/gray/frame18.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/2/gray/frame125.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/3/gray/frame139.png', './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/6/gray/frame803.png']\n",
    "    paramList = {\n",
    "                    'imgSharpening': {\n",
    "                        'n': 1\n",
    "                    },\n",
    "                    'kmeans_1': {\n",
    "                        'n_clusters': 30,\n",
    "                        'random_state': 0\n",
    "                    },\n",
    "                    'bilateralFilter': {\n",
    "                        'd': 9,\n",
    "                        'sigmaColor': 500,\n",
    "                        'sigmaSpace': 1000\n",
    "                    },\n",
    "                    'slic': {\n",
    "                        'compactness': 2000,\n",
    "                        'n_segments': 8000\n",
    "                    },\n",
    "                    'kmeans_2': {\n",
    "                        'n_clusters': 30,\n",
    "                        'random_state': 0\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    for sample in sample_data:\n",
    "        imgFileName = sample\n",
    "        tempFileName = './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/' + (imgFileName.split('/')[-1]).split('.')[0] + '_temp.png'\n",
    "        finalFileName = './' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/' + (imgFileName.split('/')[-1]).split('.')[0] + '_aoi.png'\n",
    "        \n",
    "        estimateAOIBoundingBoxes(imgFileName, tempFileName, finalFileName, paramList, False)\n",
    "        \n",
    "        logFile = open('./' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/AutomaticAOIBoundingBoxesEstimationPSNR.txt', \"a+\")\n",
    "        logFile.write(\"RMSE Value For \" + (imgFileName.split('/')[-1]).split('.')[0] + \": \" + str(RMSE(rgb2gray(io.imread(imgFileName)), rgb2gray(io.imread(finalFileName)))) + \"\\n\")\n",
    "        logFile.write(\"PSNR Value For \" + (imgFileName.split('/')[-1]).split('.')[0] + \": \" + str(PSNR(rgb2gray(io.imread(imgFileName)), rgb2gray(io.imread(finalFileName)))) + \" dB\\n\")\n",
    "        logFile.close()\n",
    "    \n",
    "    logFile = open('./' + 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_basictest/AutomaticAOIBoundingBoxesEstimationBenchmarks.txt', \"a+\")\n",
    "    logFile.write(\"\\n\")\n",
    "    logFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOverallThesis():\n",
    "    videos = VideoFrameSceneGroupingMultipleVideos()\n",
    "\n",
    "    videos.addVideo('ADDU-DI02 video_export_01-23-17-19.33.08.avi', 'ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_final')\n",
    "    videos.addVideo('ADDU-DI03 video_export_01-23-17-15.48.12.avi', 'ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_final')\n",
    "    videos.addVideo('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_final')\n",
    "\n",
    "    videos.setUseHeuristics('ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_final', True)\n",
    "    videos.setUseHeuristics('ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_final', True)\n",
    "    videos.setUseHeuristics('ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_final', True)\n",
    "\n",
    "    videos.setTotalFrames('ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_final', 850)\n",
    "    videos.setTotalFrames('ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_final', 850)\n",
    "    videos.setTotalFrames('ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_final', 850)\n",
    "\n",
    "    videos.runVideoFrameSceneGrouping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOverallThesisAOIFailSafeDebug():\n",
    "    videos = VideoFrameSceneGroupingMultipleVideos()\n",
    "\n",
    "    videos.addVideo('ADDU-DI02 video_export_01-23-17-19.33.08.avi', 'ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_final')\n",
    "    videos.addVideo('ADDU-DI03 video_export_01-23-17-15.48.12.avi', 'ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_final')\n",
    "    videos.addVideo('ADDU-DI04 video_export_01-23-17-16.15.10.avi', 'ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_final')\n",
    "\n",
    "    videos.setAOIFailSafeDebugOn('ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_final', True)\n",
    "    videos.setAOIFailSafeDebugOn('ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_final', True)\n",
    "    videos.setAOIFailSafeDebugOn('ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_final', True)\n",
    "\n",
    "    videos.setTotalFrames('ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_final', 50)\n",
    "    videos.setTotalFrames('ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_final', 50)\n",
    "    videos.setTotalFrames('ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_final', 50)\n",
    "    \n",
    "    videos.setUseHeuristics('ADDU-DI02 video_export_01-23-17-19.33.08.avi'.split(' ')[0] + '_final', True)\n",
    "    videos.setUseHeuristics('ADDU-DI03 video_export_01-23-17-15.48.12.avi'.split(' ')[0] + '_final', True)\n",
    "    videos.setUseHeuristics('ADDU-DI04 video_export_01-23-17-16.15.10.avi'.split(' ')[0] + '_final', True)\n",
    "\n",
    "    videos.runVideoFrameSceneGrouping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ezekieladrieldlagmay/Documents/Dynamic Individual Eye-Tracking/AutomaticAOIBoundingBoxes-V2/SamplePhotos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:7: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  return rgb2gray(img)\n",
      "<ipython-input-3-651e12f0caac>:40: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  labels = seg.slic(img, compactness=compactness, n_segments=n_segments)\n",
      "<ipython-input-3-651e12f0caac>:41: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  return color.label2rgb(labels, img, kind='avg')\n"
     ]
    }
   ],
   "source": [
    "#testVideoFrameSceneGroupingTValue()\n",
    "#testVideoFrameSceneGroupingCrossVideo()\n",
    "#testVideoFrameSceneGroupingHeuristicBenchmark()\n",
    "#testVideoFrameSceneGroupingHeuristicBenchmarkTwo()\n",
    "#testVideoFrameSceneGroupingSingleVideo()\n",
    "#testAutomaticAOIBoundingBoxesEstimationAlgorithmDetailed()\n",
    "testAutomaticAOIBoundingBoxesEstimationAlgorithmBenchmark()\n",
    "#testAutomaticAOIBoundingBoxesEstimationAlgorithmPSNR()\n",
    "#testOverallThesis()\n",
    "#testOverallThesisAOIFailSafeDebug()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
